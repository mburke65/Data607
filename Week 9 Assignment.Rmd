---
title: "Week 9 Homework-Data 607"
author: "Meaghan Burke"
date: "March 27, 2018"
output: html_document
---

```{r results='hide', message=FALSE, warning=FALSE}
library(jsonlite)
library(data.table)
library(dplyr)
library(knitr)
library(ggplot2)
```

## New York Times API
The New York Times web site provides a rich set of APIs, as described here: http://developer.nytimes.com/docs
Youâ€™ll need to start by signing up for an API key.
Your task is to choose one of the New York Times APIs, construct an interface in R to read in the JSON data, and
transform it to an R dataframe.

Defined the parameters of the query (search key words, start/end data and the api key)
```{r}
# March Madnes Query:

term <- "march+madness+basketball"
begin_date <- "20180301"
end_date <- "20180331"
api.key<- "&api-key=ace27904c56848de8cf3c9a855163697"

madness_url <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,"&begin_date=",begin_date,"&end_date=",
                  end_date)
madness_url
```

Used the "fromJSON" call to work with the API generate and parse the "madness_url"call
```{r, results= 'asis'}
#Fetch the data, only gives 10 rows at a time
march_madness_query <- data.frame(fromJSON(paste0(madness_url, api.key)))%>%
                      select (-c(status, copyright, response.docs.blog,
                                 response.docs.multimedia, response.docs.headline,
                                 response.docs.keywords, response.docs.byline))
march_madness_query 
```


Find the max pages available. There is a column in the output "response.meta.hit" which provides the max number of pages with results (10 per page). Divide by 10 to find the range for a loop function.
```{r}
max_pages<-round((march_madness_query$response.meta.hits[1] / 10)-1) 
max_pages
```


Looped through the response pages and combined the results 
```{r, results= 'asis'}
responses <- list()
for(i in 0:max_pages){
  madness.search <- data.frame(fromJSON(paste0(madness_url, api.key, "&page=", i), flatten = TRUE))%>%
               select(response.docs.web_url, response.docs.source, response.docs.word_count,
                      response.docs.new_desk,response.docs.type_of_material, response.docs.pub_date)%>%
                      rename(url = response.docs.web_url,source = response.docs.source,
                             word_count = response.docs.word_count, news_desk = response.docs.new_desk, 
                             material_type = response.docs.type_of_material, publish_date = response.docs.pub_date)
  responses[[i+1]] <- madness.search 
  Sys.sleep(1) 
}
combined_responses <- rbind_pages(responses)

kable(head(combined_responses,20),  caption = "March Madness Articles")

```


Found which news publishing desks publish the most march madness articles. Unsurprisingly, the sports desks published the most articles 
```{r}
combined_responses %>% 
  group_by(news_desk) %>%
  summarize(count=n()) %>%
  filter(news_desk != "None")%>%
  mutate(percent = (count / sum(count))*100) %>%
  ggplot() +
  geom_bar(aes(y=percent, x=reorder(news_desk, -percent), fill= "tomato3"), stat = "identity") + coord_flip()+
  labs(x='News Desk', 
       y='Percent Total',
       title="Publishing Desk", 
       caption="Source: New York Times API") + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```


Explored the distribution of the publishing date. It is very interesting to see the bulk of the articles came out toward the of the month.
```{r}
combined_responses %>%
  mutate(day=gsub("T.*","",publish_date)) %>%
  group_by(day) %>%
  summarise(count=n()) %>%
  ggplot() +
  geom_bar(aes(x=reorder(day, -count), y=count), fill= "yellow", stat="identity") + coord_flip()+
  labs(x='Publishing Date', 
       y='Count',
       title="Publishing Date", 
       caption="Source: New York Times API") + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```


Explored the distribution of the sources. I found it very interesting that the associated press sourced more of the content than the NY Times.
```{r}
combined_responses %>%
  group_by(source) %>%
  summarise(count=n()) %>%
  ggplot() +
  geom_bar(aes(x=reorder(source, -count), y=count), fill= "blue", stat="identity") + coord_flip()+
  labs(x='Source', 
       y='Count',
       title="Article Source", 
       caption="Source: New York Times API") + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```
